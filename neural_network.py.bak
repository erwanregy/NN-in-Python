import math
from random import random
from keras.datasets import mnist
# import numpy as np

def sigmoid(x):
    if x >= 512: return 1.0
    elif x < -512: return 0.0
    else: return 1.0 / (1.0 + math.exp(-x))


class Node:
    def __init__(self, num_inputs):
        self.weights = []
        if num_inputs != 1:
            for weight in range(num_inputs):
                self.weights.append(random())
            self.bias = random()

    def calc_output(self, inputs):
        self.output = 0.0
        for weight, input in zip(self.weights, inputs):
            self.output += weight * input
        self.output = sigmoid(self.output + self.bias)


class Layer:
    def __init__(self, num_nodes, num_inputs):
        self.nodes = [Node(num_inputs)] * num_nodes

    def calc_outputs(self, inputs):
        self.outputs = []
        for node in self.nodes:
            node.calc_output(inputs)
            self.outputs.append(node.output)


class InputLayer:
    def __init__(self, num_nodes):
        self.nodes = [None] * num_nodes

    def feed_inputs(self, inputs):
        self.outputs = inputs


class Network:
    def __init__(self, layer_sizes):
        self.layers = [InputLayer(layer_sizes[0])]
        for num_nodes, num_inputs in zip(layer_sizes[1:], layer_sizes):
            self.layers.append(Layer(num_nodes, num_inputs))

    def feed_forward(self, inputs):
        self.layers[0].feed_inputs(inputs)
        for layer, prev_layer in zip(self.layers[1:], self.layers):
            layer.calc_outputs(prev_layer.outputs)

    def calc_cost(self, expected_outputs):
        self.cost = 0.0
        for output, expected_output in zip(self.layers[-1].outputs, expected_outputs):
            self.cost += (output - expected_output) ** 2
        num_outputs = len(self.layers[-1].nodes)
        self.cost /= num_outputs

    def forward_propagate(self, inputs, expected_outputs):
        self.feed_forward(inputs)
        self.calc_cost(expected_outputs)

    def gradient_descent(self, learning_rate, inputs, expected_outputs):
        h = 0.0001
        original_cost = self.cost
        for layer_num, layer in reversed(list(enumerate(self.layers))):
            if layer_num != 0:
                for node_num, node in enumerate(layer.nodes):
                    # weights
                    for weight_num, _ in enumerate(node.weights):
                        self.layers[layer_num].nodes[node_num].weights[weight_num] += h
                        self.forward_propagate(inputs, expected_outputs)
                        change_in_cost = self.cost - original_cost
                        cost_gradient = change_in_cost/h
                        self.layers[layer_num].nodes[node_num].weights[weight_num] -= (h + cost_gradient * learning_rate)
                    # bias
                    self.layers[layer_num].nodes[node_num].bias += h
                    self.forward_propagate(inputs, expected_outputs)
                    change_in_cost = self.cost - original_cost
                    cost_gradient = change_in_cost/h
                    self.layers[layer_num].nodes[node_num].bias -= (h + cost_gradient * learning_rate)

    def train(self, inputs, labels, iterations, learning_rate):
        num_outputs = len(self.layers[-1].nodes)
        expected_outputs = []
        for label in labels:
            expected_output = [0.0] * num_outputs
            expected_output[label] = 1.0
            expected_outputs.append(expected_output)
        print(f"iteration 0: ", end="")
        self.forward_propagate(inputs[0], expected_outputs[0])
        print(f"cost = {self.cost}")
        for iteration, input, expected_output in zip(range(iterations), inputs[1:], expected_outputs[1:]): # FIXME: does this work?
            print(f"iteration {iteration + 1}: ", end="")
            self.gradient_descent(learning_rate, input, expected_output)
            print(f"cost = {self.cost}")

    def back_propagate(self):
        None

    def save_parameters(self):
        file = open("parameters.txt", "w")
        for layer_num, layer in enumerate(self.layers[1:]):
            file.write(f"layer {layer_num+1}:\n")
            for node_num, node in enumerate(layer.nodes):
                file.write(f"    node {node_num}:\n")
                file.write("        weights:\n")
                for weight_num, weight in enumerate(node.weights):
                    file.write(f"           {weight_num}: {weight}\n")
                file.write(f"        bias: {node.bias}\n")
            file.write("\n")
        file.close()


(training_images, training_labels), (testing_images, testing_labels) = mnist.load_data()

training_images = training_images.astype("float32")/255
testing_images = testing_images.astype("float32")/255

training_images = training_images.reshape(training_images.shape[0], 28*28)
testing_images = testing_images.reshape(testing_images.shape[0], 28*28)

network = Network([28*28, 16, 16, 10])
network.train(training_images, training_labels, 50, 0.1)

save_parameters = input("Save parameters? [Y/n] ")
while save_parameters != "Y" and save_parameters != "y" and save_parameters != "" and save_parameters != "N" and save_parameters != "n":
    save_parameters = input()
if save_parameters != "Y" or save_parameters != "y" or save_parameters != "":
    network.save_parameters()
